#!/usr/bin/env python3
# Generated by MLC Compiler
# Auto-generated machine learning training script

# =====================================
# Model 1: BERT
# Backend: transformers
# =====================================

epochs = 3
batch_size = 8
learning_rate = 0.000020

# Dataset Loading
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# Load dataset
dataset = load_dataset('imdb')
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Model: BERT (transformers backend)
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# Model
model_name = 'BERT'
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Training
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=epochs,
    per_device_train_batch_size=batch_size,
    learning_rate=learning_rate,
    evaluation_strategy='epoch',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
)

trainer.train()
print('Training complete!')

# Save model
trainer.save_model('./model')
print('Model saved!')
