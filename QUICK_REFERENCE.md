# MLC Compiler - Quick Reference Guide

## ğŸ¯ 7-Phase Compilation Output

### âœ¨ What You Get

When you run `./mlc_compiler_verbose -v example.mlc`, you'll see **all 7 compilation phases** clearly displayed:

---

## ğŸ“‹ Command Cheat Sheet

| Command | Description |
|---------|-------------|
| **Build** | `make -f Makefile.verbose` |
| **Run Verbose** | `./mlc_compiler_verbose -v file.mlc` |
| **Run Normal** | `./mlc_compiler_verbose file.mlc` |
| **Quick Demo** | `./RUN_VERBOSE_DEMO.sh` |
| **Clean** | `make -f Makefile.verbose clean` |

---

## ğŸ” Phase-by-Phase Breakdown

### ğŸ”¹ **PHASE 1: LEXICAL ANALYSIS**
**What it does:** Breaks source code into tokens

**Output:**
```
[DATASET     , "dataset"            , line 1]
[STRING      , ""imdb""             , line 1]
[MODEL       , "model"              , line 3]
[IDENTIFIER  , "BERT"               , line 3]
[LBRACE      , "{"                  , line 3]
...
```

**Key Points:**
- Each token has: **type**, **lexeme**, **line number**
- Recognizes: keywords, identifiers, operators, literals

---

### ğŸ”¹ **PHASE 2: SYNTAX ANALYSIS**
**What it does:** Builds Abstract Syntax Tree (AST)

**Output:**
```
Parse Tree (AST):
program
â”œâ”€â”€ dataset_decl
â”‚   â””â”€â”€ path: "imdb"
â””â”€â”€ model_def_list
    â””â”€â”€ model_def_1
        â”œâ”€â”€ model_name: BERT
        â””â”€â”€ parameters {
            â”œâ”€â”€ epochs = 3
            â”œâ”€â”€ batch_size = 8
            â””â”€â”€ learning_rate = 0.00002
```

**Key Points:**
- Shows grammar rules applied
- Visual tree structure
- Validates syntax correctness

---

### ğŸ”¹ **PHASE 3: SEMANTIC ANALYSIS**
**What it does:** Type checking and symbol table construction

**Output:**
```
Symbol Table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Name           â”‚ Type     â”‚ Value      â”‚ Scope      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dataset        â”‚ string   â”‚ imdb       â”‚ global     â”‚
â”‚ epochs         â”‚ int      â”‚ 3          â”‚ model_BERT â”‚
â”‚ learning_rate  â”‚ float    â”‚ 0.00002    â”‚ model_BERT â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Type Checking:
  âœ“ Variable 'epochs' in scope 'model_BERT': type=int, value=3
```

**Key Points:**
- All variables tracked in symbol table
- Type inference (int, float, string)
- Scope management (global, model-specific)

---

### ğŸ”¹ **PHASE 4: INTERMEDIATE CODE GENERATION**
**What it does:** Generates 3-Address Code (TAC)

**Output:**
```
3-Address Code (TAC):
  1: t0 = LOAD_DATASET(imdb)
  2: t2 = INIT_MODEL(BERT)
  3: t3 = SET_PARAM(epochs, 3)
  4: t4 = SET_PARAM(batch_size, 8)
  5: t5 = SET_PARAM(learning_rate, 0.000020)
  6: t6 = COMPILE_MODEL(optimizer, loss_fn)
  7: t7 = TRAIN(t0, epochs)
  8: t8 = SAVE_MODEL(model_path)
```

**Key Points:**
- Platform-independent representation
- Each instruction: `result = operation(args)`
- Temporary variables (t0, t1, t2, ...)

---

### ğŸ”¹ **PHASE 5: CODE OPTIMIZATION**
**What it does:** Optimizes intermediate representation

**Output:**
```
Before Optimization:
  [8 instructions shown]

Optimizations Applied:
  âœ“ Constant propagation
  âœ“ Dead code elimination (none found)
  âœ“ Common subexpression elimination (none found)

After Optimization:
  Instructions: 8 â†’ 8 (no change - code already optimal)
```

**Key Points:**
- Shows before/after IR
- Lists optimization techniques applied
- Reduces instruction count when possible

---

### ğŸ”¹ **PHASE 6: CODE GENERATION**
**What it does:** Converts IR to target language (Python)

**Output:**
```
Backend Framework: transformers

Mapping IR to Target Code:
  Model: BERT
    IR: SET_PARAM(epochs, 3)
    â†’ Python: epochs = 3
    
    IR: SET_PARAM(batch_size, 8)
    â†’ Python: batch_size = 8

âœ… Target code written to: train.py
```

**Key Points:**
- Maps IR instructions to Python code
- Selects appropriate ML framework
- Generates executable script

---

### ğŸ”¹ **PHASE 7: CODE LINKING & ASSEMBLY**
**What it does:** Links libraries and creates final executable

**Output:**
```
External Libraries Linked:
  âœ“ transformers (NLP framework)
  âœ“ datasets (dataset library)
  âœ“ torch (backend)

Virtual Environment Setup:
  âœ“ Python venv created
  âœ“ Dependencies installed
  âœ“ Environment activated

âœ… Final Output: train.py (executable Python script)
âœ… Virtual Environment: venv/ (ready to use)
```

**Key Points:**
- Links external ML frameworks
- Sets up virtual environment
- Ready-to-run output

---

## ğŸ“Š Compilation Pipeline

```
Source Code (.mlc)
    â†“
ğŸ”¹ Phase 1: Lexical Analysis  â†’ Tokens
    â†“
ğŸ”¹ Phase 2: Syntax Analysis   â†’ AST
    â†“
ğŸ”¹ Phase 3: Semantic Analysis â†’ Symbol Table
    â†“
ğŸ”¹ Phase 4: IR Generation     â†’ 3-Address Code
    â†“
ğŸ”¹ Phase 5: Optimization      â†’ Optimized IR
    â†“
ğŸ”¹ Phase 6: Code Generation   â†’ Python Code
    â†“
ğŸ”¹ Phase 7: Linking           â†’ Executable + venv
    â†“
Final Output (train.py + venv/)
```

---

## ğŸ“ Use Cases

### For Learning
```bash
# See all phases
./mlc_compiler_verbose -v example.mlc
```

### For Production
```bash
# Minimal output
./mlc_compiler_verbose example.mlc
```

### For Debugging
```bash
# Identify which phase has errors
./mlc_compiler_verbose -v buggy_code.mlc
```

---

## ğŸ“ Example Workflows

### Workflow 1: BERT Sentiment Analysis
```mlc
dataset "imdb"

model BERT {
    epochs = 3
    batch_size = 8
    learning_rate = 0.00002
}
```

**Run:** `./mlc_compiler_verbose -v example.mlc`  
**Result:** Generates BERT training code with HuggingFace Transformers

---

### Workflow 2: Random Forest Classifier
```mlc
dataset "/data/classification.csv"

model RandomForestClassifier {
    n_estimators = 100
    max_depth = 10
}
```

**Run:** `./mlc_compiler_verbose -v forest.mlc`  
**Result:** Generates scikit-learn Random Forest code

---

### Workflow 3: ResNet Image Classification
```mlc
dataset "/data/images"

model ResNet50 {
    epochs = 20
    batch_size = 32
    learning_rate = 0.001
}
```

**Run:** `./mlc_compiler_verbose -v resnet.mlc`  
**Result:** Generates TensorFlow/Keras ResNet50 training code

---

## ğŸ¯ Key Features

âœ… **Educational** - See how compilers work phase-by-phase  
âœ… **Transparent** - Every transformation is visible  
âœ… **Debugging** - Identify errors at any stage  
âœ… **Multi-Framework** - Supports Scikit-Learn, TensorFlow, PyTorch, Transformers  
âœ… **Production Ready** - Generates runnable Python code  

---

## ğŸ”§ Troubleshooting

### Build Errors
```bash
make -f Makefile.verbose clean
make -f Makefile.verbose
```

### Parser Conflicts
- The "1 shift/reduce conflict" warning is expected and harmless

### Missing Dependencies
```bash
sudo apt install flex bison gcc python3-venv
```

---

## ğŸ“š Files Overview

| File | Purpose |
|------|---------|
| `lexer_verbose.l` | Lexical analyzer with token printing |
| `parser_verbose.y` | Parser with all phase hooks |
| `compiler_phases.c` | Phase display implementations |
| `main_verbose.c` | Entry point with verbose flag |
| `example_verbose.mlc` | Example input file |
| `train.py` | Generated output (after compilation) |

---

## ğŸš€ Next Steps

1. **Build:** `make -f Makefile.verbose`
2. **Try the demo:** `./RUN_VERBOSE_DEMO.sh`
3. **Create your own .mlc file**
4. **Compile with:** `./mlc_compiler_verbose -v yourfile.mlc`
5. **Run generated code:** `venv/bin/python train.py`

---

## ğŸ’¡ Pro Tips

- Use **verbose mode** (`-v`) for learning and debugging
- Use **normal mode** for production builds
- Check the **symbol table** to verify variable scoping
- Review the **IR code** to understand optimization opportunities
- Compare **before/after optimization** to see performance improvements

---

**Happy Compiling! ğŸ‰**
