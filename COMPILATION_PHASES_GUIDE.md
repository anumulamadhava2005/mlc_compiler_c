# MLC Compiler - Multi-Phase Compilation Guide

## Overview

This guide demonstrates all **7 phases** of the MLC compiler with detailed output for each stage.

---

## Usage

### Build the Verbose Compiler
```bash
make -f Makefile.verbose
```

### Run with Verbose Mode
```bash
./mlc_compiler_verbose -v example_verbose.mlc
```

### Quick Demo
```bash
./RUN_VERBOSE_DEMO.sh
```

---

## Example Input File (example_verbose.mlc)

```mlc
dataset "imdb"

model BERT {
    epochs = 3
    batch_size = 8
    learning_rate = 0.00002
}
```

---

## Expected Output - All 7 Phases

### ğŸ”¹ PHASE 1: LEXICAL ANALYSIS

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¹ PHASE 1: LEXICAL ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Tokens extracted:

[DATASET     , "dataset"            , line 1]
[STRING      , ""imdb""             , line 1]
[MODEL       , "model"              , line 3]
[IDENTIFIER  , "BERT"               , line 3]
[LBRACE      , "{"                  , line 3]
[IDENTIFIER  , "epochs"             , line 4]
[ASSIGN      , "="                  , line 4]
[INT         , "3"                  , line 4]
[IDENTIFIER  , "batch_size"         , line 5]
[ASSIGN      , "="                  , line 5]
[INT         , "8"                  , line 5]
[IDENTIFIER  , "learning_rate"      , line 6]
[ASSIGN      , "="                  , line 6]
[FLOAT       , "0.00002"            , line 6]
[RBRACE      , "}"                  , line 7]
```

**Explanation:**
- Each token includes: **token type**, **lexeme** (actual text), and **line number**
- The lexer recognizes keywords (`dataset`, `model`), identifiers (`BERT`, `epochs`), operators (`=`, `{`, `}`), and literals (strings, integers, floats)

---

### ğŸ”¹ PHASE 2: SYNTAX ANALYSIS

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¹ PHASE 2: SYNTAX ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Grammar Rules Applied:
  program â†’ dataset_decl model_def_list
  dataset_decl â†’ DATASET STRING
  model_def_list â†’ model_def_list model_def | model_def
  model_def â†’ MODEL ID { param_list }
  param_list â†’ param_list param | param | Îµ
  param â†’ ID = value
  value â†’ INT | FLOAT | STRING

Parse Tree (AST):
program
â”œâ”€â”€ dataset_decl
â”‚   â”œâ”€â”€ DATASET
â”‚   â””â”€â”€ path: "imdb"
â””â”€â”€ model_def_list
    â””â”€â”€ model_def_1
      â”œâ”€â”€ model_name: BERT
      â”œâ”€â”€ parameters {
      â”‚   â”œâ”€â”€ epochs = 3
      â”‚   â”œâ”€â”€ batch_size = 8
      â”‚   â”œâ”€â”€ learning_rate = 0.00002
      â”‚   â””â”€â”€ }
```

**Explanation:**
- Shows the context-free grammar rules applied during parsing
- Displays the **Abstract Syntax Tree (AST)** structure
- Verifies syntactic correctness

---

### ğŸ”¹ PHASE 3: SEMANTIC ANALYSIS

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¹ PHASE 3: SEMANTIC ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Symbol Table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Name               â”‚ Type     â”‚ Value          â”‚ Scope          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dataset            â”‚ string   â”‚ imdb           â”‚ global         â”‚
â”‚ model_name         â”‚ identifierâ”‚ BERT          â”‚ model_BERT     â”‚
â”‚ epochs             â”‚ int      â”‚ 3              â”‚ model_BERT     â”‚
â”‚ batch_size         â”‚ int      â”‚ 8              â”‚ model_BERT     â”‚
â”‚ learning_rate      â”‚ float    â”‚ 0.00002        â”‚ model_BERT     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Type Checking:
  âœ“ Variable 'dataset' in scope 'global': type=string, value=imdb
  âœ“ Variable 'model_name' in scope 'model_BERT': type=identifier, value=BERT
  âœ“ Variable 'epochs' in scope 'model_BERT': type=int, value=3
  âœ“ Variable 'batch_size' in scope 'model_BERT': type=int, value=8
  âœ“ Variable 'learning_rate' in scope 'model_BERT': type=float, value=0.00002

âœ… No type errors detected.
```

**Explanation:**
- **Symbol Table**: Stores all identifiers with their types, values, and scopes
- **Type Checking**: Validates that all variables have consistent types
- **Scope Analysis**: Ensures variables are defined in appropriate scopes

---

### ğŸ”¹ PHASE 4: INTERMEDIATE CODE GENERATION

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¹ PHASE 4: INTERMEDIATE CODE GENERATION (3-Address Code)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3-Address Code (TAC):
    1: t1 = LOAD_DATASET(imdb)
    2: t2 = INIT_MODEL(BERT)
    3: t3 = SET_PARAM(epochs, 3)
    4: t4 = SET_PARAM(batch_size, 8)
    5: t5 = SET_PARAM(learning_rate, 0.00002)
    6: t6 = COMPILE_MODEL(optimizer, loss_fn)
    7: t7 = TRAIN(t1, epochs)
    8: t8 = SAVE_MODEL(model_path)
```

**Explanation:**
- **3-Address Code (TAC)**: Intermediate representation where each instruction has at most 3 operands
- Each instruction is in the form: `result = operation(arg1, arg2)`
- Temporary variables (`t1`, `t2`, etc.) hold intermediate results
- This IR is platform-independent and easier to optimize

---

### ğŸ”¹ PHASE 5: CODE OPTIMIZATION

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¹ PHASE 5: CODE OPTIMIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Before Optimization:
    1: t1 = LOAD_DATASET(imdb)
    2: t2 = INIT_MODEL(BERT)
    3: t3 = SET_PARAM(epochs, 3)
    4: t4 = SET_PARAM(batch_size, 8)
    5: t5 = SET_PARAM(learning_rate, 0.00002)
    6: t6 = COMPILE_MODEL(optimizer, loss_fn)
    7: t7 = TRAIN(t1, epochs)
    8: t8 = SAVE_MODEL(model_path)

Optimizations Applied:
  âœ“ Constant propagation
  âœ“ Dead code elimination (none found)
  âœ“ Common subexpression elimination (none found)

After Optimization:
  Instructions: 8 â†’ 8 (no change - code already optimal)
```

**Explanation:**
- **Constant Propagation**: Replace variables with their constant values where possible
- **Dead Code Elimination**: Remove unreachable or unused code
- **Common Subexpression Elimination**: Compute repeated expressions only once
- In this case, code is already optimal, so no changes are made

---

### ğŸ”¹ PHASE 6: CODE GENERATION

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¹ PHASE 6: CODE GENERATION (Target: Python)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Backend Framework: transformers

Mapping IR to Target Code:

  Model: BERT
    IR: SET_PARAM(epochs, 3)
    â†’ Python: epochs = 3
    
    IR: SET_PARAM(batch_size, 8)
    â†’ Python: batch_size = 8
    
    IR: SET_PARAM(learning_rate, 0.00002)
    â†’ Python: learning_rate = 0.00002

âœ… Target code written to: train.py
```

**Explanation:**
- Converts intermediate representation to **target language** (Python)
- Maps IR instructions to actual Python constructs
- Selects appropriate backend framework (transformers for BERT)
- Generates executable code

---

### ğŸ”¹ PHASE 7: CODE LINKING & ASSEMBLY

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”¹ PHASE 7: CODE LINKING & ASSEMBLY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
External Libraries Linked:
  âœ“ transformers (NLP framework)
  âœ“ datasets (dataset library)
  âœ“ torch (backend)

Virtual Environment Setup:
  âœ“ Python venv created
  âœ“ Dependencies installed
  âœ“ Environment activated

âœ… Final Output: train.py (executable Python script)
âœ… Virtual Environment: venv/ (ready to use)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ‰ COMPILATION COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

To run the generated code:
  $ venv/bin/python train.py
```

**Explanation:**
- **Library Linking**: Links external ML frameworks and dependencies
- **Environment Setup**: Creates isolated Python virtual environment
- **Final Assembly**: Produces ready-to-run executable script
- All dependencies are resolved and installed

---

## Generated Output File (train.py)

```python
#!/usr/bin/env python3
# Generated by MLC Compiler
# Auto-generated machine learning training script

# =====================================
# Model 1: BERT
# Backend: transformers
# =====================================

epochs = 3
batch_size = 8
learning_rate = 0.00002

# Dataset Loading
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# Load dataset
dataset = load_dataset('imdb')
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Model: BERT (transformers backend)
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# Model
model_name = 'BERT'
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Training
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=epochs,
    per_device_train_batch_size=batch_size,
    learning_rate=learning_rate,
    evaluation_strategy='epoch',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
)

trainer.train()
print('Training complete!')

# Save model
trainer.save_model('./model')
print('Model saved!')
```

---

## Compilation Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Code    â”‚
â”‚  (.mlc file)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ğŸ”¹ PHASE 1: LEXICAL ANALYSIS
â”‚  Lexer (Flex)   â”‚ â”€â”€â”€â–º Tokens: [DATASET, "imdb", line 1], ...
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ğŸ”¹ PHASE 2: SYNTAX ANALYSIS
â”‚ Parser (Bison)  â”‚ â”€â”€â”€â–º Parse Tree / AST
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ğŸ”¹ PHASE 3: SEMANTIC ANALYSIS
â”‚ Semantic Check  â”‚ â”€â”€â”€â–º Symbol Table, Type Checking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ğŸ”¹ PHASE 4: IR GENERATION
â”‚  IR Generator   â”‚ â”€â”€â”€â–º 3-Address Code (TAC)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ğŸ”¹ PHASE 5: OPTIMIZATION
â”‚   Optimizer     â”‚ â”€â”€â”€â–º Optimized IR
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ğŸ”¹ PHASE 6: CODE GENERATION
â”‚ Code Generator  â”‚ â”€â”€â”€â–º Python Code (train.py)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ğŸ”¹ PHASE 7: LINKING & ASSEMBLY
â”‚ Linker/Assembly â”‚ â”€â”€â”€â–º Executable + venv
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Output   â”‚
â”‚  train.py       â”‚
â”‚  venv/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Command Reference

| Command | Description |
|---------|-------------|
| `make -f Makefile.verbose` | Build verbose compiler |
| `./mlc_compiler_verbose -v file.mlc` | Compile with all phases shown |
| `./mlc_compiler_verbose file.mlc` | Compile without verbose output |
| `./RUN_VERBOSE_DEMO.sh` | Run full demonstration |
| `cat train.py` | View generated Python code |
| `venv/bin/python train.py` | Run the generated training script |

---

## Key Differences: Regular vs Verbose Mode

| Feature | Regular Mode | Verbose Mode (`-v`) |
|---------|--------------|---------------------|
| Token Display | âŒ Hidden | âœ… All tokens shown |
| Parse Tree | âŒ Hidden | âœ… Visual tree shown |
| Symbol Table | âŒ Hidden | âœ… Complete table shown |
| IR Code | âŒ Hidden | âœ… 3-address code shown |
| Optimization | âŒ Silent | âœ… Before/after shown |
| Code Mapping | âŒ Hidden | âœ… IRâ†’Python mapping shown |
| Library Linking | âŒ Brief message | âœ… Detailed linking info |

---

## Educational Value

This verbose compilation mode is perfect for:

âœ… **Learning compiler design** - See how each phase transforms the code  
âœ… **Debugging** - Identify which phase causes errors  
âœ… **Understanding ML frameworks** - See how high-level configs map to framework code  
âœ… **Teaching** - Demonstrate compiler theory with real examples  

---

## Next Steps

1. **Build the compiler**: `make -f Makefile.verbose`
2. **Run the demo**: `./RUN_VERBOSE_DEMO.sh`
3. **Try your own code**: Create a `.mlc` file and compile with `-v` flag
4. **Compare outputs**: Run with and without `-v` to see the difference

---

## Files Created

- `lexer_verbose.l` - Lexer with token printing
- `parser_verbose.y` - Parser with phase integration
- `compiler_phases.h` - Phase function declarations
- `compiler_phases.c` - Phase implementations
- `main_verbose.c` - Main program with verbose flag
- `Makefile.verbose` - Build configuration
- `example_verbose.mlc` - Example input
- `RUN_VERBOSE_DEMO.sh` - Quick demo script
- `COMPILATION_PHASES_GUIDE.md` - This guide

---

**Enjoy exploring the compilation phases! ğŸš€**
